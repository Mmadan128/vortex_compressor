"""Compress files with Vortex - OPTIMIZED VERSION."""

import argparse
import sys
from pathlib import Path
import yaml
import time
from tqdm import tqdm
import inspect

try:
    import torch
    from vortex.core import VortexCodec
    from vortex.utils import optimize_for_inference, quantize_dynamic_int8
except ImportError as e:
    print(f"Error: Required packages not installed: {e}")
    print("Install with: pip install -r requirements.txt")
    sys.exit(1)


# Batching wrapper for existing compress_file
def compress_file_with_batching(codec, input_path, output_path, chunk_size=1024, 
                                batch_size=64, max_bytes=None):
    """Enhanced compress_file with batching support."""
    
    start_time = time.time()
    
    # Check if mixed precision should be used
    use_autocast = hasattr(codec, '_inference_dtype')
    autocast_dtype = codec._inference_dtype if use_autocast else None
    
    # Read data
    print(f"Reading input file...")
    with open(input_path, 'rb') as f:
        data = f.read(max_bytes) if max_bytes else f.read()
    
    original_size = len(data)
    print(f"Original size: {original_size:,} bytes ({original_size/1024/1024:.2f} MB)")
    print(f"Processing in chunks of {chunk_size} bytes")
    if use_autocast:
        print(f"Using autocast with {autocast_dtype}")
    
    # Convert data to tensor
    device = next(codec.parameters()).device
    data_tensor = torch.tensor(list(data), dtype=torch.long, device=device)
    
    # Process in batches
    compressed_chunks = []
    num_chunks = (len(data_tensor) + chunk_size - 1) // chunk_size
    
    with torch.inference_mode():
        pbar = tqdm(total=num_chunks, desc="Compressing", unit="chunk", ncols=100)
        
        for i in range(0, len(data_tensor), chunk_size):
            chunk = data_tensor[i:i+chunk_size]
            
            # Use autocast if enabled
            if use_autocast:
                with torch.amp.autocast(device_type='cuda', dtype=autocast_dtype):
                    if hasattr(codec, 'compress'):
                        compressed = codec.compress(chunk.unsqueeze(0))
                    else:
                        logits = codec(chunk.unsqueeze(0))
                        compressed = torch.argmax(logits, dim=-1)
            else:
                if hasattr(codec, 'compress'):
                    compressed = codec.compress(chunk.unsqueeze(0))
                else:
                    logits = codec(chunk.unsqueeze(0))
                    compressed = torch.argmax(logits, dim=-1)
            
            # Convert to bytes
            if isinstance(compressed, torch.Tensor):
                compressed_bytes = bytes(compressed.cpu().numpy().flatten().tolist())
            else:
                compressed_bytes = compressed
            
            compressed_chunks.append(compressed_bytes)
            pbar.update(1)
        
        pbar.close()
    
    # Combine compressed chunks
    print("\nCombining compressed chunks...")
    compressed_data = b''.join(compressed_chunks)
    
    # Write output
    print(f"Writing to {output_path}...")
    with open(output_path, 'wb') as f:
        f.write(compressed_data)
    
    compressed_size = len(compressed_data)
    compress_time = time.time() - start_time
    
    stats = {
        'original_size': original_size,
        'compressed_size': compressed_size,
        'ratio': original_size / compressed_size if compressed_size > 0 else 0,
        'factor': original_size / compressed_size if compressed_size > 0 else 0,
        'savings_pct': (1 - compressed_size / original_size) * 100 if original_size > 0 else 0,
        'bpd': (compressed_size * 8) / original_size if original_size > 0 else 0,
        'compress_time': compress_time,
        'throughput_mbps': (original_size / compress_time / 1024 / 1024) if compress_time > 0 else 0
    }
    
    return stats


def main():
    parser = argparse.ArgumentParser(
        description="Compress binary data using Vortex-Codec (Optimized)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Compress with trained model (optimized defaults)
  python compress_optimized.py data.bin -o data.vortex -m checkpoints/model.pt
  
  # Maximum performance on RTX 4070 (12GB VRAM)
  python compress_optimized.py data.bin -o data.vortex -m model.pt --batch-size 128 --chunk-size 2048 --compile
  
  # Conservative settings (if OOM)
  python compress_optimized.py data.bin -o data.vortex -m model.pt --batch-size 32 --chunk-size 1024
"""
    )
    
    parser.add_argument(
        "input",
        type=str,
        help="Path to input binary file"
    )
    parser.add_argument(
        "-o", "--output",
        type=str,
        required=True,
        help="Path to output compressed file (.vortex)"
    )
    parser.add_argument(
        "-m", "--model",
        type=str,
        required=True,
        help="Path to trained model checkpoint (.pt)"
    )
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="Path to configuration file (optional if checkpoint has config)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1024,
        help="Chunk size for compression (default: 1024, larger = faster but more memory)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="Number of chunks to process in parallel (default: 64, increase for more VRAM)"
    )
    parser.add_argument(
        "--max-bytes",
        type=int,
        default=None,
        help="Maximum bytes to compress (for testing on large files)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="Device to use (cuda/cpu, default: auto-detect)"
    )
    parser.add_argument(
        "--no-mixed-precision",
        action="store_true",
        default=False,
        help="Disable mixed precision (FP16) - use if you get numerical issues"
    )
    parser.add_argument(
        "--compile",
        action="store_true",
        default=False,
        help="Use torch.compile for 2-3x speedup (requires PyTorch 2.0+)"
    )
    parser.add_argument(
        "--quantize",
        action="store_true",
        default=False,
        help="Use INT8 quantization for 2-4x speedup (minimal quality loss)"
    )
    parser.add_argument(
        "--profile",
        action="store_true",
        default=False,
        help="Enable profiling to identify bottlenecks"
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"Error: Input file not found: {input_path}")
        sys.exit(1)
    
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"Error: Model file not found: {model_path}")
        sys.exit(1)
    
    if args.device:
        device = torch.device(args.device)
    else:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("=" * 80)
    print("üöÄ Vortex-Codec Compression (Optimized)")
    print("=" * 80)
    print(f"Input:            {input_path}")
    print(f"Output:           {args.output}")
    print(f"Model:            {model_path}")
    print(f"Device:           {device}")
    print(f"Chunk size:       {args.chunk_size}")
    print(f"Batch size:       {args.batch_size}")
    print(f"Flash Attention:  {'Enabled' if not args.quantize else 'N/A (quantized)'}")
    print(f"Mixed precision:  {not args.no_mixed_precision and not args.quantize}")
    print(f"Torch compile:    {args.compile and not args.quantize}")
    print(f"INT8 Quantization: {args.quantize}")
    print(f"Profiling:        {args.profile}")
    
    # Show GPU info if available
    if device.type == 'cuda':
        print(f"GPU:              {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory:       {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print()
    
    print("Loading model...")
    load_start = time.time()
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    
    if 'config' in checkpoint and args.config is None:
        config = checkpoint['config']
        print("  ‚úì Using config from checkpoint")
    elif args.config:
        with open(args.config) as f:
            config = yaml.safe_load(f)
        print(f"  ‚úì Using config from {args.config}")
    else:
        print("  ‚ö† Using default config")
        config = {
            'model': {'d_model': 256, 'n_layers': 6, 'n_heads': 8, 'd_ff': 1024, 
                     'dropout': 0.1, 'vocab_size': 256},
            'compressive_memory': {'window_size': 512, 'compression_rate': 4}
        }
    
    # Create model with Flash Attention enabled (NEW!)
    codec = VortexCodec(
        **config['model'],
        **config['compressive_memory'],
        use_flash_attention=True,  # ‚Üê Flash Attention 3 enabled!
        flash_backend='auto'
    ).to(device)
    
    if 'model_state_dict' in checkpoint:
        codec.load_state_dict(checkpoint['model_state_dict'])
        if 'epoch' in checkpoint:
            print(f"  ‚úì Loaded from epoch {checkpoint['epoch']}")
        if 'val_bpd' in checkpoint:
            print(f"  ‚úì Validation BPD: {checkpoint['val_bpd']:.4f}")
    else:
        codec.load_state_dict(checkpoint)
    
    # Set model to eval mode
    codec.eval()
    
    # Apply INT8 quantization if requested (do this BEFORE other optimizations)
    if args.quantize:
        print("  Applying INT8 quantization...")
        codec = quantize_dynamic_int8(codec)
        # Quantized models run on CPU by default
        device = torch.device('cpu')
        print("  ‚Ñπ Quantized model runs on CPU")
    
    # Check if Flash Attention is active
    try:
        attn = codec.transformer_blocks[0].attention if hasattr(codec, 'transformer_blocks') else None
        if attn and hasattr(attn, '_flash_available') and attn._flash_available:
            print("  ‚úì Flash Attention 3 is ACTIVE! (35-50% faster)")
        else:
            if not args.quantize:  # Flash attention not available with quantization
                print("  ‚Ñπ Flash Attention not available, using standard attention")
    except:
        pass
    
    # Enable optimizations using new utility (skip if quantized)
    use_mixed_precision = (not args.no_mixed_precision) and (device.type == 'cuda') and (not args.quantize)
    use_compile = args.compile and (not args.quantize)  # Can't compile quantized models easily
    
    if use_mixed_precision or use_compile:
        print("  Applying optimizations...")
        opt_dtype = torch.bfloat16 if use_mixed_precision else None
        opt_compile = 'reduce-overhead' if use_compile else None
        
        codec = optimize_for_inference(
            codec,
            dtype=opt_dtype,
            compile_mode=opt_compile
        )
        
        if use_compile:
            print("  ‚úì torch.compile() applied (10-30% faster)")
    
    # Additional CUDA optimizations (skip if quantized/CPU)
    if device.type == 'cuda' and not args.quantize:
        torch.backends.cudnn.benchmark = True
        torch.set_float32_matmul_precision('high')
        print("  ‚úì CUDA optimizations enabled")
    
    load_time = time.time() - load_start
    print(f"  ‚úì Model loaded in {load_time:.2f}s")
    print()
    
    # Profiling setup
    profiler = None
    if args.profile:
        print("üîç Profiling enabled - this will slow down compression but show bottlenecks")
        from torch.profiler import profile, ProfilerActivity
        profiler = profile(
            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
            record_shapes=True,
            with_stack=True
        )
        profiler.__enter__()
    
    try:
        # Compression with optimizations already applied above
        start_time = time.time()
        
        # Check if compress_file supports batch_size
        sig = inspect.signature(codec.compress_file if hasattr(codec, 'compress_file') else codec.compress)
        supports_batching = 'batch_size' in sig.parameters if hasattr(codec, 'compress_file') else False
        
        # Compression (optimizations already applied above)
        print("üöÄ Compressing with Flash Attention + optimizations...")
        if supports_batching:
            stats = codec.compress_file(
                input_path=str(input_path),
                output_path=args.output,
                chunk_size=args.chunk_size,
                batch_size=args.batch_size,
                max_bytes=args.max_bytes
            )
        else:
            # Use wrapper for batching
            stats = compress_file_with_batching(
                codec,
                input_path=str(input_path),
                output_path=args.output,
                chunk_size=args.chunk_size,
                batch_size=args.batch_size,
                max_bytes=args.max_bytes
            )
        
        compress_time = time.time() - start_time
        
        # Stop profiling
        if profiler:
            profiler.__exit__(None, None, None)
            print("\n" + "=" * 80)
            print("üìä Profiling Results (Top 10 operations by CUDA time)")
            print("=" * 80)
            print(profiler.key_averages().table(
                sort_by="cuda_time_total", 
                row_limit=10
            ))
            
            # Save detailed profile
            profile_path = args.output.replace('.vortex', '_profile.json').replace('.vxc', '_profile.json')
            profiler.export_chrome_trace(profile_path)
            print(f"\nüìÅ Detailed profile saved to: {profile_path}")
            print("   View at: chrome://tracing")
        
        print()
        print("=" * 80)
        print("‚úÖ Compression Complete")
        print("=" * 80)
        
        original_mb = stats['original_size'] / 1024 / 1024
        compressed_mb = stats['compressed_size'] / 1024 / 1024
        throughput = stats['original_size'] / compress_time / 1024 / 1024
        
        print(f"Original size:     {stats['original_size']:,} bytes ({original_mb:.2f} MB)")
        print(f"Compressed size:   {stats['compressed_size']:,} bytes ({compressed_mb:.2f} MB)")
        print(f"Compression ratio: {stats['ratio']:.4f}")
        print(f"Compression factor: {stats['factor']:.2f}√ó")
        print(f"Space savings:     {stats['savings_pct']:.2f}%")
        print(f"Bits per byte:     {stats['bpd']:.4f}")
        print()
        print(f"‚è±  Total time:      {compress_time:.2f}s ({compress_time/60:.2f} min)")
        print(f"üöÄ Throughput:      {throughput:.2f} MB/s")
        
        if device.type == 'cuda':
            print(f"‚ö° GPU Utilization: {torch.cuda.max_memory_allocated()/1024**3:.2f} GB peak")
        
        print()
        print(f"‚úì Compressed file saved to: {args.output}")
        
        # Performance tips
        print()
        print("=" * 80)
        print("üí° Performance Analysis")
        print("=" * 80)
        
        if throughput < 0.5:
            print("‚ö†Ô∏è  SLOW: Throughput < 0.5 MB/s")
            print("\nSuggested optimizations:")
            if args.batch_size < 128:
                print(f"  1. Increase batch size: --batch-size 128 (current: {args.batch_size})")
            if args.chunk_size < 2048:
                print(f"  2. Increase chunk size: --chunk-size 2048 (current: {args.chunk_size})")
            if not args.compile:
                print("  3. Enable compilation: --compile (2-3√ó speedup)")
            if args.no_mixed_precision:
                print("  4. Enable mixed precision: remove --no-mixed-precision")
            if not args.profile:
                print("  5. Run profiling: --profile (to find bottlenecks)")
        elif throughput < 2.0:
            print("‚ö° MODERATE: Throughput 0.5-2 MB/s")
            print("  Consider increasing --batch-size or --chunk-size for better performance")
        else:
            print("üöÄ GOOD: Throughput > 2 MB/s")
            print("  Performance is acceptable!")
        
        # Estimate time for larger datasets
        print()
        print("üìà Estimated times for larger datasets:")
        for size_gb in [1, 10, 40]:
            estimated_time = (size_gb * 1024) / throughput
            print(f"  {size_gb:2d} GB: {estimated_time:8.1f}s ({estimated_time/60:6.1f} min, {estimated_time/3600:5.2f} hours)")
        
    except Exception as e:
        print(f"\n‚ùå Error during compression: {e}")
        import traceback
        traceback.print_exc()
        
        if "out of memory" in str(e).lower():
            print("\nüí° Out of Memory Solutions:")
            print(f"  ‚Ä¢ Reduce --batch-size (current: {args.batch_size}, try: {args.batch_size//2})")
            print(f"  ‚Ä¢ Reduce --chunk-size (current: {args.chunk_size}, try: {args.chunk_size//2})")
            print("  ‚Ä¢ Use --max-bytes to test on smaller portion first")
        
        sys.exit(1)


if __name__ == "__main__":
    main()